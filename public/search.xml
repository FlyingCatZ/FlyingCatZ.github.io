<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hexo将Blog转到Linux]]></title>
    <url>%2F2021%2F04%2F15%2Fhexo%E5%B0%86Blog%E8%BD%AC%E5%88%B0Linux%2F</url>
    <content type="text"><![CDATA[记录将blog从windows搬到linux 安装软件1234sudo apt-get install -y gitsudo apt-get install -y nodejssudo apt-get install -y build-essentialsudo apt-get install -y npm 配置Git12jian@host:~$ ssh -T git@github.comHi FlyingCatZ! You&apos;ve successfully authenticated, but GitHub does not provide shell access. &emsp;&emsp;如果当前github上没有当前linux系统的ssh秘匙就需要生成并添加 1ssh-keygen -t rsa -C &quot;邮箱地址&quot; &emsp;&emsp;接着敲３次回车，完成之后在/home/xxx/.ssh/下面可以看到秘钥，将其添加到github上即可 拷贝源文件&emsp;&emsp;核心文件是这些文件和文件夹 123456_config.ymlpackage.jsonnode_modulesscaffoldssourcethemes 安装hexo和相关模块1sudo npm install hexo-cli -g &emsp;&emsp;遇到错误，使用--force参数强制写入覆盖 12345npm installnpm install hexo-deployer-git --save // 文章部署到 git 的模块（下面为选择安装）npm install hexo-generator-feed --save // 建立 RSS 订阅npm install hexo-generator-sitemap --save // 建立站点地图 &emsp;&emsp;大功告成 参考：https://smelond.com/2018/06/21/hexo%E4%BB%8Ewindows%E6%90%AC%E5%AE%B6%E5%88%B0deepin/]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[display subsystem]]></title>
    <url>%2F2021%2F03%2F05%2Fdisplay%2F</url>
    <content type="text"><![CDATA[前言&emsp;&emsp;分析AM57xx系列架构显示原理，分别从硬件和软件两方面入手。总体看来，AM57xx系列芯片在硬件上面将显示部分分成了几大子系统，每个子系统负责相应的部分，而显示最基础的子系统就是DSS（Display Subsystem），它负责将frame buffer中图像数据真正的显示在显示器上。目前需求仅为显示静态图像，所以只分析这个子系统。 一. 显示基础&emsp;&emsp;显示最基础的部件就是显示器/显示屏，而显示器由显示面板（display panel）和显示驱动器（display driver）组成，display panel负责发光，这也就是显示器的显示原理，根据其显示原理的不同，可以分为CRT、LED、OLED、LCD等显示器，其中CRT为阴极射线管发光显示，主要应用为上世纪的电视机显示，LED利用发光二极管显示，主要应用在广场中广告屏的显示，OLED为有机发光二极管，是目前娱乐设备的主要显示载体，主要用于超薄柔性显示，而LCD是液晶显示，目前主流的显示器，应用在各行各业。而display driver一是负责接收显示数据，二是控制控制面板发光。 &emsp;&emsp;连接显示器的接口叫display interface，目前主流的display interface有VGA、DVI、HDMI、DP、雷电等接口，首先是VGA接口，CPU使用的是TTL电平，通过VGA接口则直接连接，因为其传输的是模拟信号。DVI传输的是数字信号，高分辨率下更加清晰并且失真会更少。HDMI接口（High Definition Multimedia Interface）是一种全数字化影像和声音传送接口，可以传送未压缩的音讯及视频信号，目前最流行的接口。DP接口（Display Port）具有上面所有显示器接口的一切优点，但目前成本较高。雷电接口融合了PCI Express和DisplayPort接口两种通信协议，PCI Express用于数据传输，DisplayPort用于显示，能同步传输1080p乃至4K视频和最多八声道音频，最高可达到40Gbps。 &emsp;&emsp;MIPI （Mobile Industry Processor Interface） 是2003年由ARM， Nokia， ST ，TI等公司成立的一个联盟，目的是把手机内部的接口如摄像头、显示屏接口、射频/基带接口等标准化，从而减少手机设计的复杂程度和增加设计灵活性。MIPI信号是成对传输的，主要是为了减少干扰，MIPI信号成对走线，两根线从波形看是成反相，所以有外部干扰过来，就会被抵消很大部分。主要用在平板和手机上使用。 &emsp;&emsp;MIPI接口LCD包括1对差分时钟（CLKP，CLKN），4对数据差分线（D0P，D0N；D1P，D1N；D2P，D2N；D3P，D3N），每一对之间有GND线，4对数据差分线并不一定要全部使用，很多屏只需要2对就可以了；RESET（复位脚），STBYB（高电平有效），VGL，VGH（像素点上开关管的开启关闭电压，加在开关管的栅极上，VGH 高电平打开给像素点电容充电， VGL 负电压 关闭开关管），VCOM（ 液晶像素点的存储电容共用电极），VLED-（背光负极），VLED+（背光正极），电源有1.8V和3.3V。 &emsp;&emsp;MIPI的液晶数据传输中涉及到是DWG（Display Working Group）工作组，该工作组提出了4种液晶规范分别为DCS（Display Command Set）、DBI（Display Bus Interface）、DPI（Display Pixel Interface）、DSI（Display Serial Interface）。DPI接口也可称为RGB接口，DBI接口可称为MCU接口 MIPI DCS（Display Command Set） &emsp;&emsp;规范中规定了显示命令设置的一些规范，它并没有说明它具体的硬件连接方式，规定了液晶传输中各个命令的值和意义以及命令说明，主要是为了配合DBI规范、DSI规范来使用的。 MIPI DBI（Display Bus Interface） &emsp;&emsp;规范中规定了它的硬件接口方式，它是液晶数据总线接口，可细分为MIPI DBI Type A、MIPI DBI TypeB、MIPI DBI Type C这三种不同的模式，不同模式下的硬件接口以及数据的采样都有所不同，如在MIPI DBI Type A规范中规定是下降沿采样数据值（摩托罗拉6800接口 ），MIPI DBI Type B规范中规定是上升沿采样数据（英特尔8080接口 ）。&emsp;&emsp;MIPI DBI Type A和MIPI DBI Type B同时又可细分为5种不同数据接口模式，分别为8位数据接口、9位数据接口、16位数据接口、18位数据接口、24位数据接口。不过市面上支持9位数据接口的液晶驱动IC并不多见，当然数据接口越大那么相同一个周期内数据接口越大，所传输的数据越多。而MIPI DBI Type C 只适用于传输于DCS规范中规定的命令和该命令所需要的参数值，不能传输液晶像素的颜色值（虽然DBI规范中规定能传输颜色值，不过市面上的液晶驱动IC是用来传输命令和命令所需的参数值）。&emsp;&emsp;同样在DBI（Display Bus Interface）规范中规定不同数据接口所支持颜色位数。具体还是要参考所使用的液晶驱动IC资料来确定。&emsp;&emsp;谈到颜色位数，需要说一下何谓颜色位数，颜色位数也称色彩位数，位图或者视频帧缓冲区中储存1像素的颜色所用的位数,它也称为位/像素(bpp)。色彩深度越高,可用的颜色就越多。市面常用液晶驱动IC支持的颜色位数有16、18、24这三种。 MIPI DPI（Display Pixel Interface） &emsp;&emsp;规范中所规定的硬件接口跟DBI规范中并不相同，它不是像DBI规范用Command/Data配置液晶驱动IC的寄存器再进行操作。某种程度上，DPI与DBI的最大差别是DPI的数据线和控制线分离，而DBI是复用的。同样使用DBI接口的液晶很少有大屏幕的，因为需要更多的GRAM从而提高了生产成本，而DPI接口即不需要，因为它是直接写屏，速度快，常用于显示视频或动画用。&emsp;&emsp;DPI从它的名称中就可以看出它是直接对液晶的各像素点进行操作的，它是利用（H，V）这两个行场信号进行对各像素点进行颜色填充操作。填充速度快，可用于动画显示，目前手机液晶屏所用的接口就是这一类。H（H-SYNC）称为行同步信号；V（V-SYNC）称为场同步信号。它像模拟电视机那样用电子枪那样进行扫频显示，不过它对时序控制要求很高。因此一般的MCU芯片很难支持。 MIPI DSI(Display Serial Interface) 符合MIPI协定的串列显示器界面协议，主机与显示器之间用差分信号线连接。一对clock信号和1~4对data信号一般情况下data0可以配置成双向传输一个主机端可以允许同时与多个从属端进行通信 &emsp;&emsp;最后就是display controller，也就是显示控制器，显示控制器如果在系统中配置使用了，则与其他设备一样挂载到总线上，最后，三者关系如下 二. AM57xx DSS&emsp;&emsp;AM57xx系列芯片都有一个显示子系统DSS（Display Subsystem），总体架构为 &emsp;&emsp;DSS主要由DISPC（Display controller）和HDMI protocol engine组成，DISPC又由DMA、LCD/TV outputs、GFX（graphics pipeline）、video pipelines、write-back pipeline组成。 2.1 DISPC&emsp;&emsp;在显示过程中必须得去配置DISPC使其工作起来，五个管道（pipelines）中，VIDx和GFX负责图像数据的输出，WB负责数据的反馈以进行图像数据的处理，三个LCD outputs则负责将输入的ARGB32-8888格式像素数据转换成 RGB24-888 或 YUV4:2:2 格式像素数据，TV out负责将ARGB40-10.10.10.10格式像素数据直接输出，支持MIPI DPI协议。 ​ 数据的源头都是通过DMA搬运，节省CPU的开销。架构为： 2.2 HDMI&emsp;&emsp;HDMI总体架构： &emsp;&emsp;当DISPC处理好数据格式，将数据发送给HDMI模块，而HDMI模块再将数据传送给HDMI_PHY，HDMI_PHY负责将数据输出显示，当配置HDMI接口时需要配置HDCP、HDMI模块、HDMI_PHY、PLLTRL_HDMI四个模块才能使其工作。工作时也需遵循HDMI接口标准。 三. 显示子系统3.1 总览&emsp;&emsp;显示子系统是Linux系统中最复杂的子系统之一，因为其操作的复杂性，GPU工作的特殊性和重要性，导致整个显示子系统的层次关系很多，我们只关注kernel部分。 &emsp;&emsp;在Linux内核中对于显示部分的驱动被分成了两部分，一部分是gpu目录下的显卡的驱动，另一部分是video目录下视频相关的驱动，二者都是基于frame buffer（帧缓存），在gpu/目录中，最外层的各种drm_xxx文件实现了DRI（Direct Render Infrastructure），通过这些接口能够直接访问底层的图形设备，例如LCDC、GPU等，而具体的硬件驱动在更具体的下一级目录中。通过Makefile文件可以梳理出DRM架构各个文件之间的关系： 1234567891011121314151617drm-y := drm_auth.o drm_bufs.o drm_cache.o \ drm_context.o drm_dma.o \ drm_fops.o drm_gem.o drm_ioctl.o drm_irq.o \ drm_lock.o drm_memory.o drm_drv.o drm_vm.o \ drm_scatter.o drm_pci.o \ drm_platform.o drm_sysfs.o drm_hashtab.o drm_mm.o \ drm_crtc.o drm_modes.o drm_edid.o \ drm_info.o drm_debugfs.o drm_encoder_slave.o \ drm_trace_points.o drm_global.o drm_prime.o \ drm_rect.o drm_vma_manager.o drm_flip_work.o \ drm_modeset_lock.o drm_atomic.o drm_bridge.odrm-$(CONFIG_DRM_GEM_CMA_HELPER) += drm_gem_cma_helper.odrm-$(CONFIG_PCI) += ati_pcigart.odrm-$(CONFIG_OF) += drm_of.odrm-y += $(drm-m) &emsp;&emsp;内核中的DRM为X server或Mesa 实现了操作操作硬件的接口，从而保证图像数据传输的低延迟。在同一文件夹下还存在着另外一种架构，这种架构主要是将用于控制显示设备属性的操作提供给上层直接使用，KMS（Kernel Mode Set）就是为了实现这种操作。在gpu/drm/目录中，实现KMS的文件有： 123456drm_kms_helper-y := drm_crtc_helper.o drm_dp_helper.o drm_probe_helper.o \ drm_plane_helper.o drm_dp_mst_topology.o drm_atomic_helper.odrm_kms_helper-$(CONFIG_DRM_FBDEV_EMULATION) += drm_fb_helper.odrm_kms_helper-$(CONFIG_DRM_KMS_CMA_HELPER) += drm_fb_cma_helper.oobj-$(CONFIG_DRM_KMS_HELPER) += drm_kms_helper.o &emsp;&emsp;通过DRM和KMS的封装，他们向上提供接口，向下协调硬件驱动。下面分析AM57xx系列芯片的显示驱动，同样是Makefile文件： 12345obj-y += omapdrm/obj-y += tilcdc/obj-y += i2c/obj-y += panel/obj-y += bridge/ &emsp;&emsp;其中，omapdrm/目录中实现AM57xx系列芯片上面对应的显示子系统，tilcdc/目录则是LCD controller下面抽象层次的实现，i2c/、panel/、bridge/目录则是关系具体的显示驱动底层的代码，默认的是NXP_TDA998X，后续需要根据具体显示设备具体分析，各个目录中的显示驱动程序所需要调用通用函数则都是在当前目录中实现，在当前目录还实现了DRM的核心层。 3.2 DRM&emsp;&emsp;DRM（Direct Render Manager）站在所有图形驱动的上层，为图形驱动程序提供了多种服务，同时向上通过libdrm提供应用程序接口，libdrm是包装大多数DRM ioctl的库。DRM提供的服务包括vblank事件处理，内存管理，输出管理，帧缓冲区管理，命令提交和防护，挂起/恢复支持以及DMA传输。 &emsp;&emsp;其驱动代码在gpu/omapdrm/omap_drv.c中，主要结构是drm_driver 123456789101112131415161718192021222324252627282930313233343536static struct drm_driver omap_drm_driver = &#123; .driver_features = DRIVER_MODESET | DRIVER_GEM | DRIVER_PRIME | DRIVER_ATOMIC | DRIVER_RENDER, .load = dev_load, .unload = dev_unload, .open = dev_open, .lastclose = dev_lastclose, .preclose = dev_preclose, .postclose = dev_postclose, .set_busid = drm_platform_set_busid, .get_vblank_counter = drm_vblank_no_hw_counter, .enable_vblank = omap_irq_enable_vblank, .disable_vblank = omap_irq_disable_vblank,#ifdef CONFIG_DEBUG_FS .debugfs_init = omap_debugfs_init, .debugfs_cleanup = omap_debugfs_cleanup,#endif .prime_handle_to_fd = drm_gem_prime_handle_to_fd, .prime_fd_to_handle = drm_gem_prime_fd_to_handle, .gem_prime_export = omap_gem_prime_export, .gem_prime_import = omap_gem_prime_import, .gem_free_object = omap_gem_free_object, .gem_vm_ops = &amp;omap_gem_vm_ops, .dumb_create = omap_gem_dumb_create, .dumb_map_offset = omap_gem_dumb_map_offset, .dumb_destroy = drm_gem_dumb_destroy, .ioctls = ioctls, .num_ioctls = DRM_OMAP_NUM_IOCTLS, .fops = &amp;omapdriver_fops, .name = DRIVER_NAME, .desc = DRIVER_DESC, .date = DRIVER_DATE, .major = DRIVER_MAJOR, .minor = DRIVER_MINOR, .patchlevel = DRIVER_PATCHLEVEL,&#125;; &emsp;&emsp;在Linux系统中需要大量的图形内存来存储与图形有关的数据，因此内存管理在DRM中至关重要，而且在DRM基础架构中发挥着核心作用。在DRM的内存管理核心子模块中包含两个内存管理器，Translation Table Manager（TTM）和Graphics Execution Manager（GEM）。 &emsp;&emsp;TTM提供一个单一的用户空间API，可以满足所用硬件的要求，同时支持统一内存体系结构（UMA）设备和具有专用视频RAM的设备，同时也导致代码庞大而复杂。GEM为应对TTM的复杂性，没有为每个与图形内存相关的问题提供解决方案，而是确定驱动程序之间的通用代码，并创建一个共享的支持库，从而使初始化和执行要求更简单，但是不具有RAM管理功能，也仅限于UMA设备。 &emsp;&emsp;vma-manager负责将依赖于驱动程序的任意内存区域映射到线性用户地址空间。 &emsp;&emsp;PRIME是drm中的跨设备缓冲区共享框架，对于用户空间，PRIME缓冲区是基于dma-buf的文件描述符。 &emsp;&emsp;drm_mm提供了一个简单的范围分配器。如果驱动程序合适的话，可以自由使用Linux内核中的资源分配器，drm_mm的好处是它位于DRM内核中，这意味着可以更容易满足gpu的一些特殊用途需求。 3.3 KMS&emsp;&emsp;KMS通过frame buffer提供给用户空间，而frame buffer结构嵌入到plane（面）结构中构成KMS的基本对象，面结构用drm_plane表示，之后plane再将像素数据传入crtc。crtc代表整个显示管道，从drm_plane接收像素数据，并将数据混合到一起，之后crtc将数据输出到多个编码器，用drm_encoder表示，当crtc在运行时则至少有一个drm_encoder，每个编码器再将数据输出到连接器，drm_connector，连接器与编码器的连接可以通过软件指定。一个编码器可以驱动多个连接器，但一个连接器只能有一个编码器。 &emsp;&emsp;为了能够共享编码器的代码，可以将一个或多个Framebuffer GEM Helper Reference（由struct drm_bridge表示）链接到编码器。该链接是静态的，无法更改，这意味着需要在CRTC和任何编码器之间打开交叉映射开关。另一个对象是面板（drm_panel），它的存在是为了以某种形式显示像素的其他东西，通常嵌入到连接器中。 &emsp;&emsp;最后，通过连接器抽象实际的接收器，暴露给用户空间，通过这些KMS对象来完成数据的转换和输出。 3.4 dss&emsp;&emsp;在drm/的顶层目录下的很多文件就是为了实现DRM和KMS架构，根据Makefile文件得知，DRM架构也需要i2c/、panel/、bridge/等核心模块支撑，其中panel/实现DRM面板驱动程序，最多需要一个调节器和一个GPIO才能运行，i2c/中的驱动是为了那些需要I2C协议的编码器，bridge/中的代码则是为了特殊display架构的需要。 &emsp;&emsp;最后，剩下的便是各大厂家的驱动，以TI为例，TI实现了两种显示驱动的框架，一种是以LCDC作为显示控制器的显示架构，分布在tilcdc/中。另一种则是以gpu作为显示控制器的显示架构，分布在omapdrm/中。 &emsp;&emsp;AM57xx平台采用gpu作为显示控制器，所以其代码在omapdrm/中，在omapdrm/顶层目录的代码实现DRM架构所需要的内存管理、中断、帧缓存等核心操作，同时在这个层次实现KMS架构中子模块的驱动，包括plane、crtc、encoder、connector。从drm_driver结构中可以看到omap_drm_driver所确定的操作，omap_drm_driver的操作会向下调用具体的函数，这些都以函数指针的形式调用。 &emsp;&emsp;init函数中去注册platform_drivers结 12345678910111213141516171819202122232425262728293031323334353637383940static struct platform_driver * const drivers[] = &#123; &amp;omap_dmm_driver, &amp;pdev,&#125;;struct platform_driver omap_dmm_driver = &#123; .probe = omap_dmm_probe, .remove = omap_dmm_remove, .driver = &#123; .owner = THIS_MODULE, .name = DMM_DRIVER_NAME, .of_match_table = of_match_ptr(dmm_of_match), .pm = &amp;omap_dmm_pm_ops, &#125;,&#125;;static struct platform_driver pdev = &#123; .driver = &#123; .name = DRIVER_NAME, .pm = &amp;omapdrm_pm_ops, &#125;, .probe = pdev_probe, .remove = pdev_remove,&#125;;static const struct of_device_id dmm_of_match[] = &#123; &#123; .compatible = "ti,omap4-dmm", .data = &amp;dmm_omap4_platform_data, &#125;, &#123; .compatible = "ti,omap5-dmm", .data = &amp;dmm_omap5_platform_data, &#125;, &#123; .compatible = "ti,dra7-dmm", .data = &amp;dmm_dra7_platform_data, &#125;, &#123;&#125;,&#125;; &emsp;&emsp;dss在设备树中的节点： 12345678910111213141516171819202122232425262728293031323334353637dss@58000000 &#123; compatible = "ti,dra7-dss"; status = "ok"; ti,hwmods = "dss_core"; syscon-pll-ctrl = &lt;0x8 0x538&gt;; #address-cells = &lt;0x1&gt;; #size-cells = &lt;0x1&gt;; ranges; reg = &lt;0x58000000 0x80 0x58004054 0x4 0x58004300 0x20 0x58009054 0x4 0x58009300 0x20&gt;; reg-names = "dss", "pll1_clkctrl", "pll1", "pll2_clkctrl", "pll2"; clocks = &lt;0x10f 0x110 0x111&gt;; clock-names = "fck", "video1_clk", "video2_clk"; vdda_video-supply = &lt;0x112&gt;; dispc@58001000 &#123; compatible = "ti,dra7-dispc"; reg = &lt;0x58001000 0x1000&gt;; interrupts = &lt;0x0 0x14 0x4&gt;; ti,hwmods = "dss_dispc"; clocks = &lt;0x10f&gt;; clock-names = "fck"; syscon-pol = &lt;0x8 0x534&gt;; &#125;; encoder@58060000 &#123; compatible = "ti,dra7-hdmi"; reg = &lt;0x58040000 0x200 0x58040200 0x80 0x58040300 0x80 0x58060000 0x19000&gt;; reg-names = "wp", "pll", "phy", "core"; interrupts = &lt;0x0 0x60 0x4&gt;; status = "disabled"; ti,hwmods = "dss_hdmi"; clocks = &lt;0x113 0x114&gt;; clock-names = "fck", "sys_clk"; dmas = &lt;0xd3 0x4c&gt;; dma-names = "audio_tx"; &#125;; &#125;; &emsp;&emsp;dss/目录中实现了TI DSS显示子系统的驱动，包括其中的dispc、hdmi engine以及支持的接口，在omap2系列平台中支持dpi接口、dsi接口、rfbi接口、venc接口、sdi接口，omap4以及以上平台支持hdmi接口，它们的驱动分别在具体的文件中，根据CONFIG_OMAPx_DSS_xxx来决定使用哪个接口。AM57xx平台当前配置为DPI接口和hdmi接口。 &emsp;&emsp;DSS部分的代码可以分成四部分，omapdss_boot_init、omapdss_base、omapdss、omapdss6 omapdss_boot_init 这部分代码主要进行初始化，根据从设备树上匹配的dss节点进行配置数据，主要是通过“ti,dra7-dss”属性找到dss节点，再遍历其中的子节点 1234567891011121314151617181920212223242526272829303132333435363738394041static const struct of_device_id omapdss_of_match[] __initconst = &#123; &#123; .compatible = "ti,omap2-dss", &#125;, &#123; .compatible = "ti,omap3-dss", &#125;, &#123; .compatible = "ti,omap4-dss", &#125;, &#123; .compatible = "ti,omap5-dss", &#125;, &#123; .compatible = "ti,dra7-dss", &#125;, &#123; .compatible = "ti,k2g-dss", &#125;, &#123;&#125;,&#125;;static int __init omapdss_boot_init(void)&#123; struct device_node *dss, *child; INIT_LIST_HEAD(&amp;dss_conv_list); dss = of_find_matching_node(NULL, omapdss_of_match); omapdss_walk_device(dss, true); for_each_available_child_of_node(dss, child) &#123; if (!of_find_property(child, "compatible", NULL)) continue; omapdss_walk_device(child, true); &#125; while (!list_empty(&amp;dss_conv_list)) &#123; struct dss_conv_node *n; n = list_first_entry(&amp;dss_conv_list, struct dss_conv_node, list); if (!n-&gt;root) omapdss_omapify_node(n-&gt;node); list_del(&amp;n-&gt;list); of_node_put(n-&gt;node); kfree(n); &#125; return 0;&#125; omapdss_base base部分代码由四部分组成，base、display、dss-of、output，每一部分都是实现DRM的基础，所以也是dss的基础，只列举其中的两个函数 123456789101112131415161718192021222324252627282930313233343536static void omapdss_walk_device(struct device *dev, struct device_node *node, bool dss_core)&#123; struct device_node *n; struct omapdss_comp_node *comp = devm_kzalloc(dev, sizeof(*comp), GFP_KERNEL); n = of_get_child_by_name(node, "ports"); of_node_put(n); n = NULL; while ((n = of_graph_get_next_endpoint(node, n)) != NULL) &#123; struct device_node *pn = of_graph_get_remote_port_parent(n); if (!pn) continue; if (!of_device_is_available(pn) || omapdss_list_contains(pn)) &#123; of_node_put(pn); continue; &#125; omapdss_walk_device(dev, pn, false); &#125;&#125;bool omapdss_stack_is_ready(void)&#123; struct omapdss_comp_node *comp; list_for_each_entry(comp, &amp;omapdss_comp_list, list) &#123; if (!omapdss_component_is_loaded(comp)) return false; &#125; return true;&#125; omapdss 这部分是TI dss架构的核心驱动代码，为了能够支持更多的设备和方便管理，这里同样抽象出核心层来对具体的驱动进行管理，这部分主要是dispc驱动代码以及具体的接口的驱动 1234567891011121314151617181920212223242526272829static const struct of_device_id dispc_of_match[] = &#123; &#123; .compatible = "ti,omap2-dispc", &#125;, &#123; .compatible = "ti,omap3-dispc", &#125;, &#123; .compatible = "ti,omap4-dispc", &#125;, &#123; .compatible = "ti,omap5-dispc", &#125;, &#123; .compatible = "ti,dra7-dispc", &#125;, &#123;&#125;,&#125;;static struct platform_driver omap_dispchw_driver = &#123; .probe = dispc_probe, .remove = dispc_remove, .driver = &#123; .name = "omapdss_dispc", .pm = &amp;dispc_pm_ops, .of_match_table = dispc_of_match, .suppress_bind_attrs = true, &#125;,&#125;;int __init dispc_init_platform_driver(void)&#123; return platform_driver_register(&amp;omap_dispchw_driver);&#125;void dispc_uninit_platform_driver(void)&#123; platform_driver_unregister(&amp;omap_dispchw_driver);&#125; omapdss6 对比可以发现，omapdss6是TI为了对自己家新平台k2g的支持，原理和前面dss架构相同 12345678910static struct platform_driver dss6_driver = &#123; .probe = dss6_probe, .remove = dss6_remove, .driver = &#123; .name = "omap_dss6", .pm = &amp;dss6_pm_ops, .of_match_table = dss6_of_match, .suppress_bind_attrs = true, &#125;,&#125;; 3.5 displays&emsp;&emsp;display/目录下都是和具体硬件相关的驱动代码，由encoder、connector、panel三部分组成，根据配置CONFIG_DISPLAY_xxx_xx决定使用哪个具体的驱动，如果没有对应的型号可以自己编写对应的驱动 1234567891011121314151617181920obj-$(CONFIG_DISPLAY_ENCODER_OPA362) += encoder-opa362.oobj-$(CONFIG_DISPLAY_ENCODER_TFP410) += encoder-tfp410.oobj-$(CONFIG_DISPLAY_ENCODER_TPD12S015) += encoder-tpd12s015.oobj-$(CONFIG_DISPLAY_DRA7EVM_ENCODER_TPD12S015) += dra7-evm-encoder-tpd12s015.oobj-$(CONFIG_DISPLAY_ENCODER_SII9022) += encoder-sii9022.oencoder-sii9022-y += encoder-sii9022-video.oencoder-sii9022-$(CONFIG_DISPLAY_ENCODER_SII9022_AUDIO_CODEC) += encoder-sii9022-audio.oobj-$(CONFIG_DISPLAY_ENCODER_TC358768) += encoder-tc358768.oobj-$(CONFIG_DISPLAY_CONNECTOR_DVI) += connector-dvi.oobj-$(CONFIG_DISPLAY_CONNECTOR_HDMI) += connector-hdmi.oobj-$(CONFIG_DISPLAY_CONNECTOR_ANALOG_TV) += connector-analog-tv.oobj-$(CONFIG_DISPLAY_PANEL_DPI) += panel-dpi.oobj-$(CONFIG_DISPLAY_PANEL_DSI_CM) += panel-dsi-cm.oobj-$(CONFIG_DISPLAY_PANEL_SONY_ACX565AKM) += panel-sony-acx565akm.oobj-$(CONFIG_DISPLAY_PANEL_LGPHILIPS_LB035Q02) += panel-lgphilips-lb035q02.oobj-$(CONFIG_DISPLAY_PANEL_SHARP_LS037V7DW01) += panel-sharp-ls037v7dw01.oobj-$(CONFIG_DISPLAY_PANEL_TPO_TD028TTEC1) += panel-tpo-td028ttec1.oobj-$(CONFIG_DISPLAY_PANEL_TPO_TD043MTEA1) += panel-tpo-td043mtea1.oobj-$(CONFIG_DISPLAY_PANEL_NEC_NL8048HL11) += panel-nec-nl8048hl11.oobj-$(CONFIG_DISPLAY_PANEL_TLC59108) += panel-tlc59108.o &emsp;&emsp;displays部分代码包含三部分，encoder、connector、panel，encoder部分包含了三种编码器，tpd12s015、sii9022、tc358768，connector使用hdmi，panel使用dpi encoder： 12345678910111213141516171819202122232425262728293031static struct i2c_driver sii9022_driver = &#123; .driver = &#123; .name = "sii9022", .owner = THIS_MODULE, .of_match_table = sii9022_of_match, &#125;, .probe = sii9022_probe, .remove = sii9022_remove, .id_table = sii9022_id,&#125;;static struct platform_driver tpd_driver = &#123; .probe = tpd_probe, .remove = __exit_p(tpd_remove), .driver = &#123; .name = "tpd12s015", .of_match_table = tpd_of_match, .suppress_bind_attrs = true, &#125;,&#125;;static struct i2c_driver tc358768_i2c_driver = &#123; .driver = &#123; .owner = THIS_MODULE, .name = TC358768_NAME, .of_match_table = tc358768_of_match, &#125;, .id_table = tc358768_id, .probe = tc358768_i2c_probe, .remove = tc358768_i2c_remove,&#125;; &emsp;&emsp;connector &amp; panel： 123456789101112131415161718192021222324252627282930static struct platform_driver hdmi_connector_driver = &#123; .probe = hdmic_probe, .remove = __exit_p(hdmic_remove), .driver = &#123; .name = "connector-hdmi", .of_match_table = hdmic_of_match, .suppress_bind_attrs = true, &#125;,&#125;;static struct platform_driver panel_dpi_driver = &#123; .probe = panel_dpi_probe, .remove = __exit_p(panel_dpi_remove), .driver = &#123; .name = "panel-dpi", .of_match_table = panel_dpi_of_match, .suppress_bind_attrs = true, &#125;,&#125;;static struct i2c_driver tlc59108_i2c_driver = &#123; .driver = &#123; .owner = THIS_MODULE, .name = TLC_NAME, .of_match_table = tlc59108_of_match, &#125;, .id_table = tlc59108_id, .probe = tlc59108_i2c_probe, .remove = tlc59108_i2c_remove,&#125;; &emsp;&emsp;最终，pixel数据通过connector和panel转换成屏幕设备可以识别的格式，再通过自身的解码器将图像数据显示在屏幕上。 四. 驱动分析omap_drm_driver&emsp;&emsp;下面具体分析gpu/目录下整个TI SOC显示架构驱动，在最顶层是drm/、vga/、host1x/、ipu-v3/四个目录，其中drm/和vga/是无条件必须支持的模块，但vga模块这里没有使用到，因为我们使用的是MIPI DPI接口以及HDMI接口，DRM则是支撑整个显示子系统的核心，ipu模块则需要根据硬件平台是否启用， host1x模块是DMA引擎，用于对Tegra的图形和多媒体相关模块进行寄存器访问。所以重点在drm/目录。 &emsp;&emsp;在drm/的顶层目录下的很多文件就是为了实现DRM和KMS架构，根据Makefile文件得知，DRM架构也需要i2c/、panel/、bridge/等核心模块支撑，其中panel/实现DRM面板驱动程序，最多需要一个调节器和一个GPIO才能运行，i2c/中的驱动是为了那些需要I2C协议的编码器，bridge/中的代码则是为了特殊display架构的需要。 &emsp;&emsp;最后，剩下的便是各大厂家的驱动，以TI为例，TI实现了两种显示驱动的框架，一种是以LCDC作为显示控制器的显示架构，分布在tilcdc/中。另一种则是以gpu作为显示控制器的显示架构，分布在omapdrm/中。 &emsp;&emsp;AM57xx平台采用gpu作为显示控制器，所以其代码在omapdrm/中，在omapdrm/顶层目录的代码实现DRM架构所需要的内存管理、中断、帧缓存等核心操作，同时在这个层次实现KMS架构中子模块的驱动，包括plane、crtc、encoder、connector。从drm_driver结构中可以看到omap_drm_driver所确定的操作，omap_drm_driver的操作会向下调用具体的函数，这些都以函数指针的形式调用。 &emsp;&emsp;init函数中去注册platform_drivers结构 12345678910111213141516171819202122232425262728293031323334353637383940static struct platform_driver * const drivers[] = &#123; &amp;omap_dmm_driver, &amp;pdev,&#125;;struct platform_driver omap_dmm_driver = &#123; .probe = omap_dmm_probe, .remove = omap_dmm_remove, .driver = &#123; .owner = THIS_MODULE, .name = DMM_DRIVER_NAME, .of_match_table = of_match_ptr(dmm_of_match), .pm = &amp;omap_dmm_pm_ops, &#125;,&#125;;static struct platform_driver pdev = &#123; .driver = &#123; .name = DRIVER_NAME, .pm = &amp;omapdrm_pm_ops, &#125;, .probe = pdev_probe, .remove = pdev_remove,&#125;;static const struct of_device_id dmm_of_match[] = &#123; &#123; .compatible = "ti,omap4-dmm", .data = &amp;dmm_omap4_platform_data, &#125;, &#123; .compatible = "ti,omap5-dmm", .data = &amp;dmm_omap5_platform_data, &#125;, &#123; .compatible = "ti,dra7-dmm", .data = &amp;dmm_dra7_platform_data, &#125;, &#123;&#125;,&#125;; &emsp;&emsp;因为没有具体硬件，这里所分析的都是默认的设备，具体应该根据实际硬件选择具体的设备和驱动程序。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>显示驱动</tag>
        <tag>display</tag>
        <tag>LCD</tag>
        <tag>驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dma]]></title>
    <url>%2F2021%2F03%2F04%2Fdma%2F</url>
    <content type="text"><![CDATA[前言&emsp;&emsp;Linux中关于dma的操作非常常见，一些dma驱动独立到drivers/dma/目录下，架构相关的dma操作放在arch/arm/mm/目录下，还有在drivers/base/目录下也有关于dma的驱动，以及drivers/dma-buf/目录下所形成的dma-buf架构，因此有必要梳理一下dma的框架 &emsp;&emsp;DMA（Direct Memory Access）用于在设备和内存之间直接进行数据传输而不经过CPU的一种方式，主要通过DMA控制器来实现，而DMA控制器也主要分为两种，一种是嵌入到SOC上的外部DMA控制器，另一种是设备自带的DMA控制器。 &emsp;&emsp;首先是SOC上的DMA控制器，这个控制器主要是为了解决那些自身不带DMA控制器的设备也能进行DMA传输，其他的设备通过向这个总DMA控制器注册来实现DMA传输。 &emsp;&emsp;而对于那些自身携带DMA控制器的设备来说，他们可以独自实现DMA传输。 &emsp;&emsp;其中，Descriptor描述DMA传输过程中的各种属性。DMA传输使用的是物理地址，而且所处理的buffer必须是物理上连续的。且CPU访问内存都是通过cache，但DMA不能访问cache，所以需要注意cache一致性，ARM架构通过clean、invalid操作来完成。在进行内存到设备传输时，要确保已经将cache中的内容写到内存上；在进行设备到内存传输时，从内存上读取数据之前需要确保将cache中对应的数据无效。 一、总体分析&emsp;&emsp;内核通常使用的地址是虚拟地址。我们调用kmalloc()、vmalloc()或者类似的接口返回的地址都是虚拟地址，保存在”void *”的变量中。虚拟内存系统（TLB、页表等）将虚拟地址（程序角度）翻译成物理地址（CPU角度），物理地址保存在“phys_addr_t”或“resource_size_t”的变量中。对于一个硬件设备上的寄存器等设备资源，内核是按照物理地址来管理的。驱动并不能直接使用这些物理地址，必须首先通过ioremap()接口将这些物理地址映射到内核虚拟地址空间上去。 &emsp;&emsp;I/O设备使用第三种地址：“总线地址”。如果设备在MMIO地址空间（MMIO是物理地址空间的子集）中有若干的寄存器，或者该设备足够的智能，可以通过DMA执行读写系统内存的操作，这些情况下，设备使用的地址就是总线地址。在某些系统中，总线地址与CPU物理地址相同，但一般来说不同。iommus和host bridge可以在物理地址和总线地址之间进行映射。 &emsp;&emsp;下图中对应了驱动程序访问总线地址的两种方案： 在设备初始化过程中，内核了解了所有的IO device及其对应的MMIO地址空间，CPU并不能通过总线地址A直接访问总线上的设备，host bridge会在MMIO（即物理地址）和总线地址之间进行mapping，因此，对于CPU，它实际上是可以通过B地址（位于MMIO地址空间）访问设备。驱动程序通过ioremap()把物理地址B映射成虚拟地址C，这时候，驱动程序就可以通过虚拟地址C来访问总线上的地址A了。 如果设备支持DMA，那么在驱动中可以通过kmalloc或者其他类似接口分配一个DMA buffer，并且返回了虚拟地址X，MMU将X地址映射成了物理地址Y，从而定位了DMA buffer在系统内存中的位置，驱动可以通过访问地址X来操作DMA buffer。但是设备不能通过X地址来访问DMA buffer，因为MMU对设备不可见，而且系统内存所在的系统总线和PCI总线属于不同的地址空间。在一些简单的系统中，设备可以通过DMA直接访问物理地址Y，但是在大多数的系统中，有一个IOMMU的硬件用来将DMA可访问的总线地址翻译成物理地址，也就是把上图中的地址Z翻译成Y。驱动在调用dma_map_single这样的接口函数的时候会传递一个虚拟地址X，在这个函数中会设定IOMMU的页表，将地址X映射到Z，并且将返回z这个总线地址。驱动可以把Z这个总线地址设定到设备上的DMA相关的寄存器中。这样，当设备发起对地址Z开始的DMA操作的时候，IOMMU可以进行地址映射，并将DMA操作定位到Y地址开始的DMA buffer。 二、DMA访问限制&emsp;&emsp;如果驱动是通过伙伴系统的接口（例如__get_free_page*()）或者类似kmalloc() or kmem_cache_alloc()这样的通用内存分配的接口来分配DMA buffer，那么这些接口函数返回的虚拟地址可以直接用于DMA mapping接口API，并通过DMA操作在外设和dma buffer中交换数据。但vmalloc()接口分配的DMA buffer不能直接使用，因为其物理内存不连续。 &emsp;&emsp;驱动中定义的全局变量如果编译到内核则可以用于DMA操作，因为全局变量位于内核的数据段或者bss段。在内核初始化的时候，会建立kernel image mapping，因此全局变量所占据的内存都是连续的，并且VA和PA是有固定偏移的线性关系，因此可以用于DMA操作。在定义这些全局变量的DMA buffer的时候，要小心的进行cacheline的对齐，并且要处理CPU和DMA controller之间的操作同步，以避免cache coherence问题。 &emsp;&emsp;如果驱动编译成模块全局变量则不能用于DMA操作，因为驱动中全局定义的DMA buffer不在内核的线性映射区域，其虚拟地址是在模块加载的时候，通过vmalloc分配，这时候DMA buffer如果大于一个page frame，那么实际上是无法保证其底层物理地址的连续性，也无法保证VA和PA的线性关系。 &emsp;&emsp;通过kmap接口返回的内存也是不可以做DMA buffer，其原理类似vmalloc。块设备I/O子系统和网络子系统在分配buffer的时候则会确保其内存是可以进行DMA操作的。 &emsp;&emsp;根据DMA buffer的特性，DMA操作有两种：一种是streaming，DMA buffer是一次性的，用完就销毁。这种DMA buffer需要自己考虑cache一致性。另外一种是DMA buffer是cache coherent的，软件实现上比较简单，更重要的是这种DMA buffer往往是静态的、长时间存在的。有些设备有DMA寻址限制，不同的硬件平台有不同的配置方式，有的平台没有限制，外设可以访问系统内存的每一个Byte，有些则不可以。 &emsp;&emsp;不同类型的DMA操作可能有有不同的寻址限制，也可能相同。如果相同，我们可以用第一组接口设定streaming和coherent两种DMA 操作的地址掩码。如果不同，可以使用第二组的接口进行设定： int dma_set_mask_and_coherent(struct device *dev, u64 mask); int dma_set_mask(struct device *dev, u64 mask); int dma_set_coherent_mask(struct device *dev, u64 mask); 三、DMA映射&emsp;&emsp;DMA映射分为两种，一种是一致性DMA映射（Consistent DMA mappings），另一种则是流式DMA映射（Streaming DMA mapping）。 一致性DMA映射 一致性DMA映射有两种特点： （1）持续使用该DMA buffer，初始化的时候map，系统结束时unmap。 （2）CPU和DMA controller在发起对DMA buffer的并行访问的时候不需要考虑cache操作，CPU和DMA controller都可以看到对方对DMA buffer的更新。 流式DMA映射 流式DMA映射是一次性的，一般是在DMA传输的时候才进行map，一旦DMA传输完成就立刻unmap。 &emsp;&emsp;可以看到，cmem驱动中所采用的就是这种一致性DMA映射。通过dma_alloc_coherent()函数接口分配并映射了一个较大（page大小或类似）的coherent DMA memory。其中dev参数就是执行该设备的struct device对象的，size参数指明了需要分配DMA buffer的大小，以字节为单位，dma参数为返回的总线地址，最后一个参数为分配内存的标志，返回的参数为此块buffer的虚拟地址，供CPU使用。 &emsp;&emsp;dma_alloc_coherent()函数所申请的内存是PAGE_SIZE对齐的，以PAGE_SIZE为单位申请buffer，而且此函数可以运行在进程上下文和中断上下文。 &emsp;&emsp;当所申请的buffer已经使用完，需要取消映射并释放此块内存，dma_free_coherent()函数直接取消内存的映射并释放内存，其中第三个参数为内存的虚拟地址，第四个参数为bus addr，与dma_alloc_coherent()函数不同的是，dma_free_coherent()函数只能运行在进程上下文而不能运行在中断上下文，在某些平台释放DMAbuffer的时候会引发TLB维护的操作，从而引起cpu core之间的通信，如果关闭了IRQ会锁死在SMP IPI的代码中。 &emsp;&emsp;在所申请的大块内存中还会分成很多个pool，这里是通过堆相关的函数来进行管理的，通过HeapMem_alloc()函数从大块内存中申请一个pool，HeapMem_free()则释放一个pool，具体不继续分析。 &emsp;&emsp;这里继续分析流式DMA映射的接口函数，流式DMA映射有两个版本的接口函数，一种是用来map/umap单个dma buffer，另一种用来map/umap形成scatterlist的多个dma buffer。 单个dma buffer映射 &emsp;&emsp;映射单个dma buffer的接口函数为dma_map_single()，传入的参数为struct device设备结构，虚拟地址，内存大小以及DMA操作的方向。 1dma_handle = dma_map_single(dev, addr, size, direction); &emsp;&emsp;umap单个dma buffer使用dma_unmap_single()接口函数 1dma_unmap_single(dev, dma_handle, size, direction); 多个形成scatterlist的dma buffer &emsp;&emsp;在scatterlist的情况下，需要映射的对象是分散的若干段dma buffer，通过dma_map_sg将scatterlist结构中的多个dma buffer映射成一个大块的、连续的bus address region。 1234567int i, count = dma_map_sg(dev, sglist, nents, direction);struct scatterlist *sg;for_each_sg(sglist, sg, count, i) &#123; hw_address[i] = sg_dma_address(sg); hw_len[i] = sg_dma_len(sg); &#125; &emsp;&emsp;umap多个形成scatterlist的dma buffer是通过下面的接口实现的 1dma_unmap_sg(dev, sglist, nents, direction); &emsp;&emsp;调用dma_unmap_sg的时候要确保DMA操作已经完成，另外，传递给dma_unmap_sg的nents参数需要等于传递给dma_map_sg的nents参数，而不是该函数返回的count。 &emsp;&emsp;执行流式DMA映射的时候需要考虑CPU和设备之间数据的同步问题，以保证设备看到的数据和CPU看到的数据是一样的。所以，在进行映射DMA映射，完成传输之后，需要调用相关的函数来进行同步 123dma_sync_single_for_cpu(dev, dma_handle, size, direction);//或者dma_sync_sg_for_cpu(dev, sglist, nents, direction); &emsp;&emsp;由于DMA地址空间在某些CPU架构上是有限的，因此分配并map可能会产生错误，所以需要判断过程中是否产生了错误以及出错之后的处理 检查dma_map_single和dma_map_page返回的dma address 123456dma_addr_t dma_handle;dma_handle = dma_map_single(dev, addr, size, direction); if (dma_mapping_error(dev, dma_handle)) &#123; goto map_error_handling; &#125; 当在mapping多个page的时候，如果中间发生了mapping error，那么需要对那些已经mapped的page进行unmap的操作 123456dma_addr_t dma_handle1; dma_handle1 = dma_map_single(dev, addr, size, direction); if (dma_mapping_error(dev, dma_handle1)) &#123; goto map_error_handling1; &#125; 四、DMA驱动分析以及初始化配置&emsp;&emsp;上面只分析了DMA的执行流程，但是其初始化过程以及驱动的配置方案全都没有分析，接下来会继续分析剩下的部分。下图为DMA框架的大体流程： ​ &emsp;&emsp;硬件环境为ARMv7架构，SOC为TI的AM5728，SOC上内置一个DMA控制器。Linux内核中对DMA的支持通过DMA ENGINE架构，具体的实现分为Provider、Consumer以及DMA Buffer三个方面。三种抽象为： &emsp;&emsp;Provider：就是指SOC上的DMA Controller &emsp;&emsp;Consumer：那些能利用DMA搬移数据的片上外设，例如MMC、USB Controller等 &emsp;&emsp;DMA Buffer：DMA传输过程中需要用到的数据缓冲 4.1 Provider&emsp;&emsp;Provider所抽象的是SOC上的DMA控制器，它的驱动实现是与具体架构相关，以及传输过程中cache同步问题都在架构相关的文件中，涉及到的文件主要有arch/arm/mm/dma-mapping.c、arch/arm/kernel/dma.c、arch/arm/mach-omap2/dma.c、arch/arm/plat-omap/dma.c、drivers/base/dma-mapping.c、drivers/base/dma-coherent.c、drivers/base/*、drivers/dma/*等文件 arch/arm/mm/dma-mapping.c：主要实现由上层传来的分配buffer、从CMA区域分配buffer、带cache操作的分配buffer等操作的具体实现 arch/arm/kernel/dma.c：主要实现dma channel以及channel的各种操作，包括分配channel、释放channel等，其中还包括在procfs中创建接口 arch/arm/mach-omap2/dma.c：为设备树文件解析出来的plat-form节点分配内存并映射到内存中，初始化其中的部分数据 arch/arm/plat-omap/dma.c：解析出来的plat-form节点驱动和设备节点的初始化以及注册到内核，还包括中断的处理和注册 drivers/base/dma-mapping.c：对base目录下的coherent和contiguous两个关于dma文件的抽象，相当于一个核心层 drivers/base/dma-coherent.c：对于CMA及其他关于连续内存的操作 drivers/dma/omap-dma.c：dma engine驱动的具体实现，根据具体硬件SOC上的DMA控制器实现相应的驱动，包括omap dma驱动、dma-crossbar驱动、virt-dma驱动等 drivers/dma/dmaengine.c：抽象出的dmaengine架构，在上层将各种dma控制器的驱动抽象到一起，构成一层核心层 4.2 Consumer&emsp;&emsp;Consumer则是利用DMA进行传输的其他外设，他们通过dmaengine提供的统一的接口去调用更底层的DMA驱动，如上图中的最上层就是提供给Consumer使用的。Consumer作为slave端，需要遵守一定的规则去进行DMA传输： 分配一个DMA slave channel 设置slave和DMA控制器特殊的参数 获取一个描述DMA传输的descriptor 提交传输 发出DMA请求并等待反馈信息 4.3 DMA Buffer&emsp;&emsp;DMA传输根据方向可以分为device to memory、memory to device、device to device、memory to memory四种，其中memory to memory有自己专有的一套API，以async_开头，最后，因为mem2mem的DMA传输有了比较简洁的API，没必要直接使用dma engine提供的API，最后就导致dma engine所提供的API就特指为Slave-DMA API（即其他三种DMA传输） &emsp;&emsp;当传输的源或者目的地是memory的时候，为了提高效率，DMA controller不会每一次传输都访问memory，而是在内部开一个buffer，将数据缓存在自己buffer中： memory是源的时候，一次从memory读出一批数据保存在自己的buffer中，然后再一点点（以时钟为节拍）传输到目的地 memory是目的地的时候，先将源的数据传输到自己的buffer中，当累计到一定数量之后，再一次性写入memory DMA控制器内部可缓存的数据量的大小称作burst size &emsp;&emsp; 一般的DMA控制器只能访问物理地址连续的内存，但在有些场景下，我们只有一些物理地址不连续的内存块，需要DMA把这些内存块的数据搬移到别处，这种场景称为scatter-gather。 &emsp;&emsp;实现scatter-gather也有两种方式，一种是在DMA核心层提供scatter-gather的能力，用软件去模拟。这种方式需要先将内存块的数据搬移到一个连续的地址，然后让DMA从这个新地址开始搬移。另一种是DMA控制器本身支持scatter-gather，直接配置控制器即可，在软件上需要准备一个table或link-list，这里不继续深入分析。 五代码分析&emsp;&emsp;linux内核版本4.4.19，分析的方向为自底向上，从最底层架构相关到DMA驱动最后到其他驱动调用DMA接口 5.1 架构相关&emsp;&emsp;在DMA相关的操作中，有关架构的操作和系统初始化是先于设备初始化的，系统初始化阶段会完成底层架构操作与base层的绑定，具体流程大致为 &emsp;&emsp;其中，在初始化过程中就会完成DMA操作的定义，主要是完成DMA控制器与架构相关操作的实现，通过上层的调用能够执行最底层的DMA操作，当在驱动中去调用DMA的接口函数时，则直接调用与底层架构相关的函数接口，完成所需动作，具体函数调用流程为： &emsp;&emsp;这个过程主要是实现最底层的DMA操作，其中最主要的就是arm_dma_ops结构体的实现和注册 &emsp;&emsp;先分析arm_dma_alloc函数，它主要是获取DMA所需的buffer，这里需要先声明一些关于页表的类型和操作，所有的物理页面都是4k对齐的，因此所有表项的地址只需要高20位，而低12位则用于记录页面的状态信息和访问权限，即pgprot_t类型。 &emsp;&emsp;这里主要是执行第二个函数__dma_alloc，根据设备的不同，所分配的页面位置和页面类型也是不同的，如果只是普通的分配页面则执行simple_buffer的分配，如果是CMA内存区域则直接从所保留的内存区域分配页面，CMA的分析参考上一篇，如果是流式DMA buffer则和普通的页面分配是一样的，还有一种从pool中分配页面和remap页面，暂不分析其用途 &emsp;&emsp;我们这里是流式DMA，所以所分配的buffer是通过__alloc_simple_buffer函数，传入的参数分别为设备节点、buffer大小、页面标志，__alloc_simple_buffer则继续向下调用__dma_alloc_buffer，其最终通过底层页分配器的接口–alloc_pages实现buffer的分配 &emsp;&emsp;页分配器的工作原理后续再分析，其他函数的实现也暂不继续分析 5.2 DMA驱动&emsp;&emsp;首先看DMA对应在设备树中的节点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657... sdma_xbar: dma-router@b78 &#123; compatible = "ti,dra7-dma-crossbar"; reg = &lt;0xb78 0xfc&gt;; #dma-cells = &lt;1&gt;; dma-requests = &lt;205&gt;; ti,dma-safe-map = &lt;0&gt;; dma-masters = &lt;&amp;sdma&gt;; &#125;; edma_xbar: dma-router@c78 &#123; compatible = "ti,dra7-dma-crossbar"; reg = &lt;0xc78 0x7c&gt;; #dma-cells = &lt;2&gt;; dma-requests = &lt;204&gt;; ti,dma-safe-map = &lt;0&gt;; dma-masters = &lt;&amp;edma&gt;; &#125;;... sdma: dma-controller@4a056000 &#123; compatible = "ti,omap4430-sdma"; reg = &lt;0x4a056000 0x1000&gt;; interrupts = &lt;GIC_SPI 7 IRQ_TYPE_LEVEL_HIGH&gt;, &lt;GIC_SPI 8 IRQ_TYPE_LEVEL_HIGH&gt;, &lt;GIC_SPI 9 IRQ_TYPE_LEVEL_HIGH&gt;, &lt;GIC_SPI 10 IRQ_TYPE_LEVEL_HIGH&gt;; #dma-cells = &lt;1&gt;; dma-channels = &lt;32&gt;; dma-requests = &lt;127&gt;; &#125;; edma: edma@43300000 &#123; compatible = "ti,edma3-tpcc"; ti,hwmods = "tpcc"; reg = &lt;0x43300000 0x100000&gt;; reg-names = "edma3_cc"; interrupts = &lt;GIC_SPI 361 IRQ_TYPE_LEVEL_HIGH&gt;, &lt;GIC_SPI 360 IRQ_TYPE_LEVEL_HIGH&gt;, &lt;GIC_SPI 359 IRQ_TYPE_LEVEL_HIGH&gt;; interrupt-names = "edma3_ccint", "emda3_mperr", "edma3_ccerrint"; dma-requests = &lt;64&gt;; #dma-cells = &lt;2&gt;; ti,tptcs = &lt;&amp;edma_tptc0 7&gt;, &lt;&amp;edma_tptc1 0&gt;; &#125;;... uart1: serial@4806a000 &#123; compatible = "ti,dra742-uart", "ti,omap4-uart"; reg = &lt;0x4806a000 0x100&gt;; interrupts-extended = &lt;&amp;crossbar_mpu GIC_SPI 67 IRQ_TYPE_LEVEL_HIGH&gt;; ti,hwmods = "uart1"; clock-frequency = &lt;48000000&gt;; status = "disabled"; dmas = &lt;&amp;edma_xbar 49 0&gt;, &lt;&amp;edma_xbar 50 0&gt;; dma-names = "tx", "rx"; &#125;; &emsp;&emsp;在设备树中如果一个设备可以利用DMA传输，只需要在设备节点中加入dmas属性，并声明所使用的DMA控制器以及channel编号，例如uart1中所使用的edma 49和50号channel。 &emsp;&emsp;使用DMA设备有很多，为了方便管理和使用，同时也是为了利用内核中现有的驱动框架，DMA驱动的实现也是标准的总线-设备-驱动模型，在设备驱动模型中还有隐藏在幕后的kobject、class和kset，每一个kobject对应sys文件系统里的一个目录，其parent指针将形成一个树状分层结构，class则是抽象设备的高层视图，描述的是设备的集合，不包含同类型的设备的底层实现细节，kset则是kobject的顶层容器类 &emsp;&emsp;在drivers/dma/目录中与DMA驱动相关的文件主要有dmaengine.c、edma.c、of-dma.c、omap-dma.c、ti-dma-crossbar.c、virt-dma.c，dmaengine.c是整个DMA驱动的最上层入口，在这里实现了DMA驱动模型，即上面的一些结构，还抽象了一个dma_bus总线，初始化了一个pool。omap-dma.c和edma.c分别对应SOC上面的System DMA和Enhanced DMA的驱动程序，of-dma.c实现了基于DMA的一些设备树操作，ti-dma-crossbar.c则是dma-crossbar的驱动程序，virt-dma.c对应虚拟channel。 &emsp;&emsp;首先是dmaengine.c，主要是去注册创建一个pool，这个pool是通过slab分配器实现的 12345678910111213141516171819202122232425262728static int __init dmaengine_init_unmap_pool(void)&#123; int i; for (i = 0; i &lt; ARRAY_SIZE(unmap_pool); i++) &#123; struct dmaengine_unmap_pool *p = &amp;unmap_pool[i]; size_t size; size = sizeof(struct dmaengine_unmap_data) + sizeof(dma_addr_t) * p-&gt;size; /* slab分配器接口，以后分析 */ p-&gt;cache = kmem_cache_create(p-&gt;name, size, 0, SLAB_HWCACHE_ALIGN, NULL); if (!p-&gt;cache) break; /* slab分配器接口，以后分析 */ p-&gt;pool = mempool_create_slab_pool(1, p-&gt;cache); if (!p-&gt;pool) break; &#125; if (i == ARRAY_SIZE(unmap_pool)) return 0; dmaengine_destroy_unmap_pool(); return -ENOMEM;&#125; &emsp;&emsp;然后是omap-dma.c，这里是dma驱动的具体实现，其中主要是probe函数，当在dma-bus总线上匹配到dma设备就会执行probe函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104static int omap_dma_probe(struct platform_device *pdev)&#123; struct omap_dmadev *od; struct resource *res; int rc, i, irq; /* 为omap_dmadev结构体申请内存 */ od = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(*od), GFP_KERNEL); if (!od) return -ENOMEM; /* 获取内存资源 */ res = platform_get_resource(pdev, IORESOURCE_MEM, 0); od-&gt;base = devm_ioremap_resource(&amp;pdev-&gt;dev, res); if (IS_ERR(od-&gt;base)) return PTR_ERR(od-&gt;base); od-&gt;plat = omap_get_plat_info(); if (!od-&gt;plat) return -EPROBE_DEFER; /* 这里都是配置od对象 */ od-&gt;reg_map = od-&gt;plat-&gt;reg_map; dma_cap_set(DMA_SLAVE, od-&gt;ddev.cap_mask); dma_cap_set(DMA_CYCLIC, od-&gt;ddev.cap_mask); dma_cap_set(DMA_MEMCPY, od-&gt;ddev.cap_mask); od-&gt;ddev.device_alloc_chan_resources = omap_dma_alloc_chan_resources; od-&gt;ddev.device_free_chan_resources = omap_dma_free_chan_resources; od-&gt;ddev.device_tx_status = omap_dma_tx_status; od-&gt;ddev.device_issue_pending = omap_dma_issue_pending; od-&gt;ddev.device_prep_slave_sg = omap_dma_prep_slave_sg; od-&gt;ddev.device_prep_dma_cyclic = omap_dma_prep_dma_cyclic; od-&gt;ddev.device_prep_dma_memcpy = omap_dma_prep_dma_memcpy; od-&gt;ddev.device_config = omap_dma_slave_config; od-&gt;ddev.device_pause = omap_dma_pause; od-&gt;ddev.device_resume = omap_dma_resume; od-&gt;ddev.device_terminate_all = omap_dma_terminate_all; od-&gt;ddev.device_synchronize = omap_dma_synchronize; od-&gt;ddev.src_addr_widths = OMAP_DMA_BUSWIDTHS; od-&gt;ddev.dst_addr_widths = OMAP_DMA_BUSWIDTHS; od-&gt;ddev.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV); od-&gt;ddev.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST; od-&gt;ddev.dev = &amp;pdev-&gt;dev; INIT_LIST_HEAD(&amp;od-&gt;ddev.channels); spin_lock_init(&amp;od-&gt;lock); spin_lock_init(&amp;od-&gt;irq_lock); od-&gt;dma_requests = OMAP_SDMA_REQUESTS; if (pdev-&gt;dev.of_node &amp;&amp; of_property_read_u32(pdev-&gt;dev.of_node, "dma-requests", &amp;od-&gt;dma_requests)) &#123; dev_info(&amp;pdev-&gt;dev, "Missing dma-requests property, using %u.\n", OMAP_SDMA_REQUESTS); &#125; for (i = 0; i &lt; OMAP_SDMA_CHANNELS; i++) &#123; rc = omap_dma_chan_init(od); if (rc) &#123; omap_dma_free(od); return rc; &#125; &#125; /* 从设备树中获取中断 */ irq = platform_get_irq(pdev, 1); if (irq &lt;= 0) &#123; dev_info(&amp;pdev-&gt;dev, "failed to get L1 IRQ: %d\n", irq); od-&gt;legacy = true; &#125; else &#123; /* Disable all interrupts */ od-&gt;irq_enable_mask = 0; omap_dma_glbl_write(od, IRQENABLE_L1, 0); rc = devm_request_irq(&amp;pdev-&gt;dev, irq, omap_dma_irq, IRQF_SHARED, "omap-dma-engine", od); if (rc) return rc; &#125; od-&gt;ddev.filter.map = od-&gt;plat-&gt;slave_map; od-&gt;ddev.filter.mapcnt = od-&gt;plat-&gt;slavecnt; od-&gt;ddev.filter.fn = omap_dma_filter_fn; /* 注册OMAP-DMA设备 */ rc = dma_async_device_register(&amp;od-&gt;ddev); platform_set_drvdata(pdev, od); if (pdev-&gt;dev.of_node) &#123; omap_dma_info.dma_cap = od-&gt;ddev.cap_mask; /* Device-tree DMA controller registration */ rc = of_dma_controller_register(pdev-&gt;dev.of_node, of_dma_simple_xlate, &amp;omap_dma_info); if (rc) &#123; pr_warn("OMAP-DMA: failed to register DMA controller\n"); dma_async_device_unregister(&amp;od-&gt;ddev); omap_dma_free(od); &#125; &#125; dev_info(&amp;pdev-&gt;dev, "OMAP DMA engine driver\n"); return rc;&#125; &emsp;&emsp;edma驱动中涉及到edma-tptc和edma的注册，主体还是edma的probe函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170static int edma_probe(struct platform_device *pdev)&#123; struct edma_soc_info *info = pdev-&gt;dev.platform_data; s8 (*queue_priority_mapping)[2]; int i, off, ln; const s16 (*rsv_slots)[2]; const s16 (*xbar_chans)[2]; int irq; char *irq_name; struct resource *mem; struct device_node *node = pdev-&gt;dev.of_node; struct device *dev = &amp;pdev-&gt;dev; struct edma_cc *ecc; bool legacy_mode = true; int ret; if (node) &#123; const struct of_device_id *match; match = of_match_node(edma_of_ids, node); if (match &amp;&amp; (u32)match-&gt;data == EDMA_BINDING_TPCC) legacy_mode = false; info = edma_setup_info_from_dt(dev, legacy_mode); if (IS_ERR(info)) &#123; dev_err(dev, "failed to get DT data\n"); return PTR_ERR(info); &#125; &#125; pm_runtime_enable(dev); ret = pm_runtime_get_sync(dev); ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32)); ecc = devm_kzalloc(dev, sizeof(*ecc), GFP_KERNEL); ecc-&gt;dev = dev; ecc-&gt;id = pdev-&gt;id; ecc-&gt;legacy_mode = legacy_mode; /* When booting with DT the pdev-&gt;id is -1 */ if (ecc-&gt;id &lt; 0) ecc-&gt;id = 0; /* 同样获取设备的内存资源 */ mem = platform_get_resource_byname(pdev, IORESOURCE_MEM, "edma3_cc"); ecc-&gt;base = devm_ioremap_resource(dev, mem); if (IS_ERR(ecc-&gt;base)) return PTR_ERR(ecc-&gt;base); platform_set_drvdata(pdev, ecc); /* 从硬件IP中获取edma的配置参数 */ ret = edma_setup_from_hw(dev, info, ecc); /* 基于硬件IP参数申请内存 */ ecc-&gt;slave_chans = devm_kcalloc(dev, ecc-&gt;num_channels, sizeof(*ecc-&gt;slave_chans), GFP_KERNEL); ecc-&gt;slot_inuse = devm_kcalloc(dev, BITS_TO_LONGS(ecc-&gt;num_slots), sizeof(unsigned long), GFP_KERNEL); ecc-&gt;default_queue = info-&gt;default_queue; for (i = 0; i &lt; ecc-&gt;num_slots; i++) edma_write_slot(ecc, i, &amp;dummy_paramset); if (info-&gt;rsv) &#123; /* Set the reserved slots in inuse list */ rsv_slots = info-&gt;rsv-&gt;rsv_slots; if (rsv_slots) &#123; for (i = 0; rsv_slots[i][0] != -1; i++) &#123; off = rsv_slots[i][0]; ln = rsv_slots[i][1]; set_bits(off, ln, ecc-&gt;slot_inuse); &#125; &#125; &#125; /* 清除xbar在unused链表中的通道映射 */ xbar_chans = info-&gt;xbar_chans; if (xbar_chans) &#123; for (i = 0; xbar_chans[i][1] != -1; i++) &#123; off = xbar_chans[i][1]; &#125; &#125; /* 获取中断 */ irq = platform_get_irq_byname(pdev, "edma3_ccint"); if (irq &lt; 0 &amp;&amp; node) irq = irq_of_parse_and_map(node, 0); irq = platform_get_irq_byname(pdev, "edma3_ccerrint"); if (irq &lt; 0 &amp;&amp; node) irq = irq_of_parse_and_map(node, 2); if (irq &gt;= 0) &#123; irq_name = devm_kasprintf(dev, GFP_KERNEL, "%s_ccerrint", dev_name(dev)); ret = devm_request_irq(dev, irq, dma_ccerr_handler, 0, irq_name, ecc); if (ret) &#123; dev_err(dev, "CCERRINT (%d) failed --&gt; %d\n", irq, ret); return ret; &#125; &#125; ecc-&gt;dummy_slot = edma_alloc_slot(ecc, EDMA_SLOT_ANY); if (ecc-&gt;dummy_slot &lt; 0) &#123; dev_err(dev, "Can't allocate PaRAM dummy slot\n"); return ecc-&gt;dummy_slot; &#125; queue_priority_mapping = info-&gt;queue_priority_mapping; /* 事件队列优先映射 */ for (i = 0; queue_priority_mapping[i][0] != -1; i++) edma_assign_priority_to_queue(ecc, queue_priority_mapping[i][0], queue_priority_mapping[i][1]); for (i = 0; i &lt; ecc-&gt;num_region; i++) &#123; edma_write_array2(ecc, EDMA_DRAE, i, 0, 0x0); edma_write_array2(ecc, EDMA_DRAE, i, 1, 0x0); edma_write_array(ecc, EDMA_QRAE, i, 0x0); &#125; ecc-&gt;info = info; /* 初始化dma设备和channels */ edma_dma_init(ecc, legacy_mode); for (i = 0; i &lt; ecc-&gt;num_channels; i++) &#123; /* 分配所有的channels到默认的队列 */ edma_assign_channel_eventq(&amp;ecc-&gt;slave_chans[i], info-&gt;default_queue); /* 设置虚拟slot的入口位置 */ edma_set_chmap(&amp;ecc-&gt;slave_chans[i], ecc-&gt;dummy_slot); &#125; ecc-&gt;dma_slave.filter.map = info-&gt;slave_map; ecc-&gt;dma_slave.filter.mapcnt = info-&gt;slavecnt; ecc-&gt;dma_slave.filter.fn = edma_filter_fn; ret = dma_async_device_register(&amp;ecc-&gt;dma_slave); if (ret) &#123; dev_err(dev, "slave ddev registration failed (%d)\n", ret); goto err_reg1; &#125; if (ecc-&gt;dma_memcpy) &#123; ret = dma_async_device_register(ecc-&gt;dma_memcpy); if (ret) &#123; dev_err(dev, "memcpy ddev registration failed (%d)\n", ret); dma_async_device_unregister(&amp;ecc-&gt;dma_slave); goto err_reg1; &#125; &#125; if (node) of_dma_controller_register(node, of_edma_xlate, ecc); dev_info(dev, "TI EDMA DMA engine driver\n"); return 0;err_reg1: edma_free_slot(ecc, ecc-&gt;dummy_slot); return ret;&#125; &emsp;&emsp;然后是ti-dma-crossbar.c，负责dma事件映射 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116static int ti_dra7_xbar_probe(struct platform_device *pdev)&#123; struct device_node *node = pdev-&gt;dev.of_node; const struct of_device_id *match; struct device_node *dma_node; struct ti_dra7_xbar_data *xbar; struct property *prop; struct resource *res; u32 safe_val; size_t sz; void __iomem *iomem; int i, ret; if (!node) return -ENODEV; xbar = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(*xbar), GFP_KERNEL); if (!xbar) return -ENOMEM; dma_node = of_parse_phandle(node, "dma-masters", 0); if (!dma_node) &#123; dev_err(&amp;pdev-&gt;dev, "Can't get DMA master node\n"); return -ENODEV; &#125; match = of_match_node(ti_dra7_master_match, dma_node); if (!match) &#123; dev_err(&amp;pdev-&gt;dev, "DMA master is not supported\n"); return -EINVAL; &#125; if (of_property_read_u32(dma_node, "dma-requests", &amp;xbar-&gt;dma_requests)) &#123; dev_info(&amp;pdev-&gt;dev, "Missing XBAR output information, using %u.\n", TI_DRA7_XBAR_OUTPUTS); xbar-&gt;dma_requests = TI_DRA7_XBAR_OUTPUTS; &#125; of_node_put(dma_node); xbar-&gt;dma_inuse = devm_kcalloc(&amp;pdev-&gt;dev, BITS_TO_LONGS(xbar-&gt;dma_requests), sizeof(unsigned long), GFP_KERNEL); if (!xbar-&gt;dma_inuse) return -ENOMEM; if (of_property_read_u32(node, "dma-requests", &amp;xbar-&gt;xbar_requests)) &#123; dev_info(&amp;pdev-&gt;dev, "Missing XBAR input information, using %u.\n", TI_DRA7_XBAR_INPUTS); xbar-&gt;xbar_requests = TI_DRA7_XBAR_INPUTS; &#125; if (!of_property_read_u32(node, "ti,dma-safe-map", &amp;safe_val)) xbar-&gt;safe_val = (u16)safe_val; prop = of_find_property(node, "ti,reserved-dma-request-ranges", &amp;sz); if (prop) &#123; const char pname[] = "ti,reserved-dma-request-ranges"; u32 (*rsv_events)[2]; size_t nelm = sz / sizeof(*rsv_events); int i; if (!nelm) return -EINVAL; rsv_events = kcalloc(nelm, sizeof(*rsv_events), GFP_KERNEL); if (!rsv_events) return -ENOMEM; ret = of_property_read_u32_array(node, pname, (u32 *)rsv_events, nelm * 2); if (ret) return ret; for (i = 0; i &lt; nelm; i++) &#123; ti_dra7_xbar_reserve(rsv_events[i][0], rsv_events[i][1], xbar-&gt;dma_inuse); &#125; kfree(rsv_events); &#125; res = platform_get_resource(pdev, IORESOURCE_MEM, 0); iomem = devm_ioremap_resource(&amp;pdev-&gt;dev, res); if (IS_ERR(iomem)) return PTR_ERR(iomem); xbar-&gt;iomem = iomem; xbar-&gt;dmarouter.dev = &amp;pdev-&gt;dev; xbar-&gt;dmarouter.route_free = ti_dra7_xbar_free; xbar-&gt;dma_offset = (u32)match-&gt;data; mutex_init(&amp;xbar-&gt;mutex); platform_set_drvdata(pdev, xbar); /* Reset the crossbar */ for (i = 0; i &lt; xbar-&gt;dma_requests; i++) &#123; if (!test_bit(i, xbar-&gt;dma_inuse)) ti_dra7_xbar_write(xbar-&gt;iomem, i, xbar-&gt;safe_val); &#125; ret = of_dma_router_register(node, ti_dra7_xbar_route_allocate, &amp;xbar-&gt;dmarouter); if (ret) &#123; /* Restore the defaults for the crossbar */ for (i = 0; i &lt; xbar-&gt;dma_requests; i++) &#123; if (!test_bit(i, xbar-&gt;dma_inuse)) ti_dra7_xbar_write(xbar-&gt;iomem, i, i); &#125; &#125; return ret;&#125; &emsp;&emsp;最后是虚拟channel，在virt-dma.c文件中实现 123456789101112131415void vchan_init(struct virt_dma_chan *vc, struct dma_device *dmadev)&#123; dma_cookie_init(&amp;vc-&gt;chan); spin_lock_init(&amp;vc-&gt;lock); INIT_LIST_HEAD(&amp;vc-&gt;desc_allocated); INIT_LIST_HEAD(&amp;vc-&gt;desc_submitted); INIT_LIST_HEAD(&amp;vc-&gt;desc_issued); INIT_LIST_HEAD(&amp;vc-&gt;desc_completed); tasklet_init(&amp;vc-&gt;task, vchan_complete, (unsigned long)vc); vc-&gt;chan.device = dmadev; list_add_tail(&amp;vc-&gt;chan.device_node, &amp;dmadev-&gt;channels);&#125; 5.3 具体实例&emsp;&emsp;这里给出一个实际驱动中调用dma传输的一个例子，在cmem驱动中通过一致性dma接口分配了buffer，调用v7_dma_map_area函数实现cache的同步和dma传输]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>驱动</tag>
        <tag>dma</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMA]]></title>
    <url>%2F2021%2F02%2F13%2Fcma%2F</url>
    <content type="text"><![CDATA[前言&emsp;&emsp;CMA（Contiguous Memory Alloctor 连续内存分配器）是Linux内存管理子系统中的一个模块，负责物理地址连续的内存分配。一般系统会在启动过程中，从整个memory中配置一段连续内存用于CMA，然后内核其他的模块可以通过CMA的接口API进行连续内存的分配，它的底层还是依赖内核伙伴系统这样的内存管理机制。 &emsp;&emsp;问：为什么需要CMA模块？ &emsp;&emsp;在嵌入式设备中，很多设备往往需要 较大的内存缓冲区（如: 一个200万像素的高清帧摄像机，需要超过 6M 的内存)， kmalloc 内存分配机制对于这么大的内存是没有效果的。一些嵌入式设备对缓冲区 有一些额外的要求，比如： 在含有多个内存 bank 的设备中，要求只能在特定的 bank 中分配内存；而还有一些要定内存边界对齐的缓存区。近来，嵌入式设备有了较大的发展（特别是 V4L 领域），并且这些驱动都有自己的内存分配代码。CMA 框架企图采用统一的连续内存分配机制，并为这些设备驱动提供简单的API，实现定制化和模块化，而且CMA可以实现物理连续内存在不使用时这片内存能够被其他模块”借用”，需要的时候将其移走即可，还有就是： &emsp;&emsp;1.huge page（超过4k的页）模块分配 &emsp;&emsp;2.驱动需求，在嵌入式设备中如果没有IOMMU（设备访问的内存管理），而且DMA也不具备scatter/getter功能（IO分散聚集接口），这时必须通过CMA进行物理连续内存的分配 设备树配置&emsp;&emsp;设备树中对保留内存进行设置的参数在Documentation/devicetree/bindings/reserved-memory/reserved-memory.txt文档中有说明： &emsp;&emsp;在/reserved-memory节点中必须有#address-cells，#size-cells两个参数指定地址、大小参数的个数，ranges参数必须有，且为空。 &emsp;&emsp;在/reserved-memory里面的每一个子节点都可以通过两种方式来分配内存，一种为静态方式（static allocation），用reg属性指定分配内存的地址和大小。另一种为动态方式（dynamic allocation），用size属性指定大小，alignment指定对其大小，alloc-ranges指定可接受分配的内存区域，后面两个参数是可选的，第一个参数必须有。 &emsp;&emsp;子节点中还有compatible属性指定所分配内存区域是公有的还是私有的，no-map属性指定不映射到内核区域（一般用于专有驱动），也就是说初始化时不创建内存映射，由驱动进行ioremap，reusable属性指定该区域可以存储易失性数据或缓存数据，linux,cma-default属性指定使用CMA默认的池。 &emsp;&emsp;Linux4.4版本中/reserved-memory节点中保留的内存四块是给DSP和IPU用，设置为公有区域，用于与其他核通信，两块专门留给cmem驱动程序使用，默认为私有属性，只有cmem驱动能够访问。其中cmem的第一块内存为0xa0000000开始的192MB空间，第二块内存为0x40500000开始的1MB空间。 1234567891011121314151617181920reserved-memory &#123; #address-cells = &lt;0x2&gt;; #size-cells = &lt;0x2&gt;; ranges; ... cmem_block_mem@a0000000 &#123; reg = &lt;0x0 0xa0000000 0x0 0xc000000&gt;; no-map; status = "okay"; phandle = &lt;0xef&gt;; &#125;; cmem_block_mem@40500000 &#123; reg = &lt;0x0 0x40500000 0x0 0x100000&gt;; no-map; status = "okay"; phandle = &lt;0xf0&gt;; &#125;; &#125;; &emsp;&emsp;CMA预留内存的方式有三种， &emsp;&emsp;第一种：给CMA内存池分配一块固定的内存，不分配给特定的设备驱动程序，以预留的内存区域用作默认的CMA内存池。 &emsp;&emsp;第二种：预留内存给特定的设备驱动使用，通常在设备树中指定好各项参数，驱动程序通过解析设备树节点来处理内存区域的属性，并且通过物理地址和大小使用memremap / ioremap等API映射内存区域使用。 &emsp;&emsp;需要注意的是：事实上，多数设备驱动不能直接调用CMA API，因为它是在页和页帧编号（PFNs）上操作而无关总线地址和内核映射，并且也不提供维护缓存一致性的机制。 &emsp;&emsp;第三种：通过DMA API预留内存，有的时候设备驱动程序需要采用DMA的方式使用预留的内存，对于这种场景，可以将dts中节点属性设置为shared-dma-pool，从而生成为特定设备驱动程序预留的DMA内存池。设备驱动程序仅需要以常规方式使用DMA API，无需使用默认的CMA内存池。 &emsp;&emsp;一般驱动都是用第三种方式。 &emsp;&emsp;另外，配置CMA内存还可以通过命令行参数和内核Kbuild配置。 初始化过程&emsp;&emsp;这里在配置文件中配置了CONFIG_NO_BOOTMEM选项，表示完全使用memblock内存分配器代替bootmem内存分配器。memblock内存分配器是Linux内核启动过程中早期的内存分配器，主要负责从设备树上面解析内存信息，从而确定整个系统的的内存布局，通过解析设备树节点/reserved-memory读取保留的内存范围，将每块内存信息添加到memblock.reserved内存块中的数组中，memblock.reserved中的内存都是已经分配出去的内存。armv7架构中（内核4.4）调用过程为 start_kernel()-&gt;setup_arch()-&gt;setup_machine_fdt()-&gt;early_init_dt_scan() -&gt;early_init_dt_scan_nodes-&gt;early_init_dt_scan_memory() -&gt;early_init_dt_add_memory_arch()-&gt;memblock_add() &emsp;&emsp;最终通过memblock_add函数把所有内存添加到memblock.memory中，把已分配出去的内存添加到memblock.reserved中，通过memblock.memory构建起整个内存的框架。 ​ &emsp;&emsp;CMA通过在启动阶段预先保留内存，这些内存叫做CMA区域或CMA上下文，这些内存需要通过CMA接口来进行分配，在进行CMA区域的初始化之前通过early_init_fdt_scan_reserved_mem()函数保留内存，它向下调用memblock_alloc_range_nid()函数，首先调用memblock_find_in_range_node()函数遍历memblock.memory内存块中的数组从中找到指定的区域分配，在通过membloc_reserve()函数将分配出去的内存添加到memblock.reserved中，最后通过kmemleak_alloc_phys()建立CMA内存区域对象，稍后返回给伙伴系统从而可以被用作正常申请使用。 ​ &emsp;&emsp;arm_memblock_init() 函数中函数调用了 early_init_fdt_scan_reserved_mem() 函数，该函数 从 DTB 中将所有预留区的信息读取出来，然后从 MEMBLOCK 分配器中申请指定长度的物理内存，并将这些预留区加入到系统预留区数组 reserved-mem[] 进行管理，以供后期内核初始化使用。 &emsp;&emsp;在伙伴系统建立之前调用dma_contiguous_reserve函数对CMA区域进行初始化，通过cma_declare_contiguous()函数建立起CMA区域专有内存。 &emsp;&emsp;调用过程为 start_kernel()-&gt;setup_arch()-&gt;arm_memblock_init()-&gt;dma_contiguous_reserve() -&gt;dma_contiguous_reserve_area()-&gt;cma_declare_contiguous() &emsp;&emsp;输出打印： &emsp;&emsp;接下来需要对这块区域进行初始化，如果compitable设置为”shared-memory-pool”，也就是将CMA区域设置为公有区域，则会调用RESERVEDMEM_OF_DECLARE宏，在__reservedmem_of_table节中插入新的CMA区域数据，在函数中调用 __reserved_mem_init_node() 函数遍历 __reservedmem_of_table section, 该 section 内包含了 对预留区的初始化函数。 setup_arch()-&gt;arm_memblock_init()-&gt;early_init_fdt_scan_reserved_mem() -&gt;fdt_init_reserved_mem()-&gt;__reserved_mem_init_node &emsp;&emsp;输出打印： &emsp;&emsp;如果是私有的CMA区域，需要驱动程序去申请内存并进行初始化映射，而且在驱动中需要通过dma_declare_contiguous函数与对应的CMA区域绑定。 &emsp;&emsp;此时CMA区域已经构建完成，但页表还没有构建起来，需要为CMA区域构建页表，同样是对公有区域，私有区域需要驱动程序实现。在setup_arch函数中初始化完CMA区域紧接着就是paging_init函数，其为CMA建立对应的页表。它会向下继续调用dma_contiguous_remap函数，为cma_mmu_remap数组中每一个区域建立页表。 &emsp;&emsp;调用过程为 setup_arch() -&gt; paging_init() -&gt; dma_contiguous_remap() -&gt; flush_tlb_kernel_range() &amp; iotable_init() &emsp;&emsp;至此，CMA内存区域的初始化就完成了。 CMA分配器初始化&emsp;&emsp;我们需要对CMA区域中的内存进行申请、释放，这些都是通过CMA分配器实现。内核初始化过程中，通过 core_initcall() 函数将该section内的初始化函数遍历执行，其中包括 CMA 的激活入口cma_init_reserved_areas()函数， 该函数遍历分配的所有CMA分区并激活每一个CMA分区。该函数向下调用cma_activate_area()函数激活每一个区域。 &emsp;&emsp;调用过程 core_initcall(cma_init_reserved_areas) -&gt; cma_activate_area() &emsp;&emsp;在该函数中， 函数首先调用 kzalloc() 函数为CMA分区的bitmap所需的内存，然后调用init_cma_reserved_pageblock()函数。在该函数中，内核将 CMA 区块内的所有物理页都清除RESERVED标志，引用计数设置为0，接着按pageblock的方式设置区域内的页组迁移类型为MIGRATE_CMA。函数继续调用set_page_refcounted()函数将引用计数设置为1以及调用__free_pages()函数将所有的页从CMA分配器中释放并归还给buddy管理器。最后调用adjust_managed_page_count()更新系统可用物理页总数。 &emsp;&emsp;至此系统的其他部分可以开始使用CMA分配器分配的连续物理内存。 通过CMA分配连续内存&emsp;&emsp;CMA内存的分配在多数情况下不能直接被驱动程序所调用，都是通过对dma接口进行重构，实现用dma接口访问。对分配的buffer通过dma-buf实现共享，最重要的是实现零拷贝，这里还需要说明一下dma-buf： &emsp;&emsp;dma-buf是内核中的一个子系统，实现了一个让不同设备、子系统之间进行共享缓存的统一框架。本质上是 buffer 与 file 的结合，即 dma-buf 既是块物理 buffer，又是个 linux file。buffer 是内容，file 是媒介，只有通过 file 这个媒介才能实现同一 buffer 在不同驱动之间的流转。 &emsp;&emsp;dma_buf子系统包含三个主要组成: dma-buf对象，它代表的后端是一个sg_table结构，它暴露给应用层的接口是一个文件描述符，通过传递描述符达到了交互访问dma-buf对象，进而最终达成了共享访问sg_table的目的。 fence对象, 它提供了在一个设备完成访问时发出信号的机制。 reservation对象, 它负责管理缓存的分享和互斥访问。 &emsp;&emsp;在CMEM驱动中，初始化过程中调用dma_declare_contiguous()函数实现cma保留内存并对其进行初始化。它会向下调用cma_declare_contiguous()函数，从而与cma接口对接起来。 &emsp;&emsp;dma申请内存时有两种缓冲区映射方式，一种是一致性缓冲区映射，另一种是流式缓冲区映射，他们最大的区别就是一致性缓冲区映射可同时供多个设备访问，而流式缓冲区映射一次只能有一个设备访问。 &emsp;&emsp;per-device通过dma接口申请内存时，采用标准的接口dma_alloc_coherent()，通过dma_map_ops结构体间接调用dma_alloc_from_contiguous函数，从而分配内存。 &emsp;&emsp;调用过程： dma_alloc_coherent() –&gt; dma_alloc_attrs() –&gt; ops() –&gt;alloc() –&gt; arm_coherent_dma_alloc() –&gt; __dma_alloc() –&gt; __alloc_from_contiguous() –&gt; dma_alloc_from_contiguous() &emsp;&emsp;这里通过dma-buf架构使用标准dma接口dma_alloc_coherent()向下调用cma接口，这个过程是通过注册dma-buf数据结构时完成的。构建dma-buf时，需要有dma_buf_ops结构体，通过DEFINE_DMA_BUF_EXPORT_INFO宏重定义exp_info结构体，最后调用dma_buf_export()函数导出，这些操作封装在cmem_dmabuf_export()函数中。 &emsp;&emsp;分配内存API的另外一个接口是dma_alloc_from_contiguous，它是用于向下调用cma相关的操作。 &emsp;&emsp;释放内存API接口dma_release_from_contiguous &emsp;&emsp;在CMEM的驱动中还有一个小操作就是seq_file的使用，针对proc文件的不足而诞生了Seq_file，Seq_file的实现基于proc文件，作用是将Linux内核里面常用的数据结构通过文件（主要关注proc文件）导出到用户空间。 &emsp;&emsp;主要结构为： 1234567static struct file_operations cmem_proc_ops = &#123; .owner = THIS_MODULE, .open = cmem_proc_open, .read = seq_read, .llseek = seq_lseek, .release = seq_release,&#125;; &emsp;&emsp;在cmem_proc_open中调用seq_open注册cmem_seq_ops 123456static struct seq_operations cmem_seq_ops = &#123; .start = cmem_seq_start, .next = cmem_seq_next, .stop = cmem_seq_stop, .show = cmem_seq_show,&#125;; &emsp;&emsp;需要用户实现这四个函数 CMA 核心数据结构 &emsp;&emsp;struct cma 结构用于维护一块 CMA 区域， CMA 分配器维护着所有可用的 CMA 区域，每个 CMA 区域都是一段连续的物理内存。 &emsp;&emsp;cma_areas 是一个 struct cma 数组，由于维护 CMA 分配器中可用的 CMA 区域。cma_area_count 变量用于指向当前最大可用的 CMA 区域数量。 &emsp;&emsp;reserved_mem[] 数组用于维护系统早期的预留内存区。系统初始化节点会将 CMA 区域和 DMA 区域加入到该数组。reserved_mem[] 数组总共包含 MAX_RESERVED_REGIONS 个区域，reserved_mem_count 指定了最大可用的预留区数。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>内存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim & terminator使用]]></title>
    <url>%2F2021%2F02%2F13%2Ftools%2F</url>
    <content type="text"><![CDATA[前言​ 使用SourceInsight查看源代码虽然很方便，但有时也会遇到无法跳转的情况，而且修改之后还要将文件上传到linux端才能进行编译，很不方便，于是乎全面转向vim的使用 一、配置1.1 vim​ 使用vim --version查看vim版本，我的是8.2版本，为了兼容一些插件的使用，配置文件为~/.vimrc，我的文件链接vimrc，vim因为增加了插件才会如此强大，而插件也是用vim的一些语法写的，究其原因还是因为设计的灵活，使其能够扩展 ​ 我使用的插件都在文件中有说明，我是用vim-plug管理插件，主要是为了能够使用asyncrun和YouCompleteMe，所有插件如下 插件 作用 autoload_cscope 自动加载cscope相关文件 NERD_tree 文件列表，F3开关 taglist 符号列表，F2开关 asyncrun 在vim中异步操作，F7编译，F10开关状态窗口 YouCompleteMe 自动补全 ​ 前三个直接将文件放到.vim/plugin/目录中即可，后两个需要用vim-plug下载，文件全部在我的Github 1.2 terminator​ terminator的分屏特别好用，目前是我使用的主要终端工具，配置文件config，将其放到~/.config/terminator/中即可 二、效果2.1 vim 2.2 terminator]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>vim</tag>
        <tag>terminator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从学校到职场]]></title>
    <url>%2F2020%2F01%2F06%2F%E4%BB%8E%E5%AD%A6%E6%A0%A1%E5%88%B0%E8%81%8C%E5%9C%BA%2F</url>
    <content type="text"><![CDATA[以前总是期待外面的世界，可是当走出来之后才发现，最美好的地方已经回不去了。 不知不觉就过去了近六个月，也终于快迎来了自己的转正。在几个月的时间中发生了好多事情，不过还好2019年已经过去了，希望2020年继续加油。 在工作中体会还蛮多的，可能是因为自己还是太菜了，不断地努力才可以啊。这半年写了一些基于arm-a15处理器、Linux系统的代码，我知道这并不是我想要的。进入社会，就必须一刻不停地向前走。 这半年也算对产品的开发有了大致的框架，底层操作系统，应用层逻辑，上层Qt开发，arm处理器+Linux似乎是快速实现产品的必由之路。而我就是负责应用层逻辑，需要跟DSP通过IPC通信，底层大佬跟我讲IPC都是通过共享内存加中断来实现的，大佬就是大佬，言简意赅。DSP是负责控制器算法的实现和伺服驱动器的控制，主控芯片是TI的am335x，两个arm核加DSP核。还有上面的示教器，Linux加Qt，用C++实现可视化界面。每天的工作就是跟DSP端和示教器端沟通，虽然有一些进步，可是没有达到自己满意的程度。 春节过后就可以做底层开发了，这应该是这几个月最开心的事情了。加油！]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>工作</tag>
        <tag>技能</tag>
        <tag>程序员</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[嵌入式Linux_存储管理器基础]]></title>
    <url>%2F2019%2F06%2F26%2F%E5%B5%8C%E5%85%A5%E5%BC%8FLinux-%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E5%99%A8%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[从这篇开始正式进入嵌入式Linux的学习，跟着韦大神脚步，任重而道远。总结一下各种存储器。 RAM–随机存取存储器特点：易失性存储器，掉电无法使用，但写入读取速度非常快。 ROM–只读存储器特点：非易失性存储器掉电仍可使用，容量大，但写入读取速度相对没有那么快。 SRAM–静态存储器一般用于CPU的缓存，特点：写入读取速度非常快，是目前最快的存储器，但造价非常昂贵。 DRAM–动态存储器特点：保留数据的时间很短，速度也比SRAM慢，不过还是比任何的ROM都要快，从价格上来看DRAM相比SRAM要便宜很多，计算机内存就是DRAM的。 SDRAM–同步动态存储器一般用于嵌入式系统的内存，特点：利用一个单一的系统时钟同步所有的地址数据和控制信号。 DDR RAM–Double Data Rate SDRAM一般用于计算机内存，特点：在一个时钟读写两次数据，使数据传输速度加倍。 EEPROM–电擦除编程ROM特点：容量大，掉电数据不丢失，但写入速度非常慢。 FLASH–闪存U盘，MP3等存储，特点：非易失性存储器，擦写方便，访问速度快，常用来存储bootloader，操作系统，程序代码，或者直接当作硬盘。 NOR FLASH开机引导程序的载体，NOR FLASH的读取与SDRAM的读取一样，代码可以直接在NOR FLASH上执行。特点：可随机访问，容量小，读取速度快，写入和擦除速度慢（多数写入之前需要进行擦除） NAND FLASH各种存储卡，U盘，SSD（固态硬盘）等大容量设备，以page为单位读写，以block为单位擦除，代码不能直接在NAND FLASH上执行。特点：容量大，读取、写入和擦除速度都很快（多数写入之前需要进行擦除）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Arm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[科学上网]]></title>
    <url>%2F2019%2F06%2F20%2F%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[最近喜欢听下一站茶山刘，可能是因为经历相似的原因。弄科学上网也弄了几个月了，也终于摸索出了几种方法实现科学上网。（即翻墙） 引言我觉得做一件事首先是考虑一下自己的需求，如果没有需求大可不必浪费时间。 1. 软件或插件 windows： 如果只是想使用Google搜索和维基百科，可以去下载一个Chrome浏览器，安装一个叫谷歌上网助手的浏览器插件，具体安装不说了。特点非常简单，而且稳定快速。 windows： 真正的翻墙，各种vpn软件Porton，Google浏览器插件Setup，可以实现真正意义上的翻墙。特点是简单，但速度没保障。 iPhone和iPad： 软件Porton，但需要更改Appstore里appleID的地区。特点也是简单，但速度也没保障。 Android： Porton或Turbo，下载需要寻找网站或安装Google框架。 2. vps 代理服务器，基于vps可以使用ss、ssr、v2ray方式翻墙，各个平台都一样，在客户端安装shadowsocks等其他软件（我使用的是ssr），代理服务器端安装好各种方式的软件，设置好各参数（网上有一键安装脚本）即可。特点是复杂，但速度非常快。 通过vps，可以使用一些可以刷固件的路由器，在路由器上使用插件进行翻墙，只需要连接到路由器，并将dns设置一下即可，一劳永逸。俗称软路由。]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>科学上网</tag>
        <tag>翻墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大学总结]]></title>
    <url>%2F2019%2F06%2F20%2F%E5%A4%A7%E5%AD%A6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[起初打算完成毕业论文之后写一篇总结，算是给自己大学的四年一个交代。希望以后自己的执行力能更强一点。 前言 Writer就读的大学只是一所普通的二本院校。四年的学习让我真正找到了自己人生的方向，也知道了自己想要成为什么样的人。生活很美好，加油。 学习 最近几个月为了完成毕业论文，第一次认真了解了word这个软件，各种格式要求被安排的明明白白。幸亏我们的导师对我们比较了解，提前让我们写论文，后期相比其他组，我们是最轻松的。通过两个月的接触，也深深感受到了黄老师身上严谨的作风，还有就是作为一名教育工作者深深的人格魅力。 四年过的很快，依稀记得当年高考结束时的情景。而如今大学也结束了，接下来是在社会中学习。大学里的学习不再是只有课本上的知识，还有实践，生活中都需要学习。 生活 大学四年的生活有苦也有甜，有失落也有开心，这些都是成长的印记。相处了四年的室友，一起笑过，一起拼过，闹过矛盾，吵过架，依然感谢与你们相遇，现在，只能希望你们都好。 总结 最大的感受就是不会说话了。。。。。。四年更多的是与机器打交道，但我依旧怀着一颗真诚的心。学会了思考，学会了吃苦，学会了包容，学会了理解，原明天更加美好。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客注意事项]]></title>
    <url>%2F2019%2F06%2F20%2F%E5%8D%9A%E5%AE%A2%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[刚想更新一下博客，发现好多问题，以这篇博客记录一些问题 1 首先是 markdown 语法，好记性不如烂笔头 一级标题二级标题 加粗 斜体 斜体加粗 删除线 引用 分割线- - - 超链接名 百度 无序列表*_标题 有序列表1.说明 2.说明 列表说明 表头 表头 表头 内容 内容 内容 内容 内容 内容 第二行分割表头和内容, - 有一个就行,文字默认居左, - 两边加：表示文字居中, - 右边加：表示文字居右 单行代码 123代码块 2 其次是hexo和atom的一些问题 xxx command not found hexo的命令行需要使用cmd或者powershell打开，gitbash权限不够 atom的即时演示窗口快捷键 ctrl + shift + m 音乐更换文件位置 E:\Blog\themes\next\source\dist\music.js 待续]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+Github搭建个人博客网站]]></title>
    <url>%2F2019%2F04%2F16%2Fhexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[搭建自己的博客网站 准备阶段 ​ 首先，需要准备一个Github账号，直接注册即可。 ​ 安装好node.js 和 npm（只需到官网下载安装包即可，不同平台安装方法不一样），npm 会捆绑nodejs一起安装，具体看手册。 ​ 如果需要重定向域名，还需要去买一个域名，腾讯云阿里云都行，不需要可以不买，准备阶段就这些 Github创建博客站点仓库，用于存储网页相关文件 ​ 进入Github个人仓库，新建一个repo，注：不能随意命名，仓库名必须是 Github账号的名字.github.io，因为这是GitHub的一个开源项目，这样命名就是说明这个仓库用于保存你的网站文件（详情请自行参考GitHub），其他可不填 新建文件夹，安装hexo ​ 新建一个文件夹并进入文件夹，用于本地网站文件存储，这里安装hexo（开源博客框架）依赖上面安装的npm和node.js，右键打开power shell 或 Git Bash 或 cmd，npm install -g hexo-cli,等待安装成功，接着hexo init,npm install，一个博客网站就完成了（这里可能就是框架的作用，一个命令就搞定所有所需文件） 配置文件 ​ 打开文件根目录下的 _config.yml 文件，修改网站的信息，在power shell 或 Git Bash 或 cmd 中输入 hexo g 生成网站文件，hexo s 启动本地服务器可以在不联网的情况下查看网站。/themes/xxx这里的文件是主题相关的，修改之后需要将之前生成的网站文件清除掉，执行hexo clean，改完之后hexo g，一般主题都在GitHub上有说明怎么使用，具体配置参考其他博客，参考主題 部署网站 ​ 网站文件生成之后需要将网站文件传到GitHub那个刚刚建好的仓库之中，需要将本地的网站文件与Github仓库关联起来，需要将网站配置文件 _config.yml 中的deploy，修改为：1234deploy: type: git repo: git@github.com:GitHub账户名字/GitHub账户名字.github.io.git branch: master ​ 注：这里repo的链接决定上传到GitHub的方式，ssh方式更快且不需要每次都输入ID和密码，然后在主目录中输入 npm install hexo-deployer-git --save ，重新执行 hexo clean ， hexo g ，然后执行 hexo d部署网站文件，hexo会自动将网文件上传到GitHub中，然后访问GitHub账户名字.github.io.git这个站点就能访问自己的网站 更改域名 ​ 域名更改需要在/source/目录下新建一个名为 CNAME 的文件（是CNAME不是GNAME），输入你的域名（最好不带www.，这样无论带不带www.博客都能访问），并将域名解析与你的GitHub库关联起来（域名解析需要到一些平台，本人使用的腾讯云，这里不过多赘述），保存即可 写博客 ​ 主目录执行hexo n &quot;博客名字&quot;，新建了一篇博客，在顶层目录/source/_posts/目录下就会生成你的博客文件，之后才真正进行你的创作，一个好用的Markdown编辑器（我用的是Typora）可以提升体验（Markdowm语法请自行Google），博客书写遵循Markdown语法，是一种很高效的东东，可以学习一下，写完重新生成，部署，大功告成 后续可以自定义主题样式以及一些功能 ​ 笔者博客：My Blog]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>Github</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇博客]]></title>
    <url>%2F2019%2F04%2F14%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[增加菜单项 hexo new page 项目名 编辑文件 /source/项目名/index.md在date下添加 type: &quot;项目名&quot; 重新执行命令 hexo g , hexo d 或 hexo s 测试]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
